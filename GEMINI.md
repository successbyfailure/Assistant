# Assistant - Asistente Ag√©ntico para Android

## Visi√≥n General
Assistant es un asistente de voz y texto altamente configurable para Android, dise√±ado para ser r√°pido, fluido y compatible con ecosistemas abiertos. Soporta m√∫ltiples endpoints de IA (OpenAI, Ollama, LocalAI) con capacidades de streaming en tiempo real y failover autom√°tico.

## Estado del Proyecto - Fase 1.5 (‚úÖ COMPLETADA)

### Infraestructura Base
- [x] Estructura de `VoiceInteractionService` para ser Asistente Predeterminado del Sistema
- [x] `AssistantSettingsActivity` configurada como punto de entrada de configuraci√≥n
- [x] Overlay de interacci√≥n r√°pida para activaci√≥n del sistema
- [x] Cliente compatible con OpenAI con soporte de **Streaming (SSE)**
- [x] Gesti√≥n de m√∫ltiples Endpoints (OpenAI, Ollama, LocalAI, APIs compatibles)
- [x] Configuraci√≥n por categor√≠as (**Agent, STT, TTS, ImageGen, OCR**)
- [x] Sistema de **Failover** autom√°tico (Modelo Principal y de Respaldo)
- [x] Carga din√°mica de modelos desde el servidor

### Interfaz de Usuario
- [x] **ChatActivity** como pantalla principal con interfaz conversacional
- [x] Men√∫ de tres puntos (‚ãÆ) para acceso a configuraci√≥n y opciones
- [x] RecyclerView con adaptador de chat para historial de mensajes
- [x] Manejo correcto de **WindowInsets** para edge-to-edge display
- [x] Soporte para navegaci√≥n gestural (Pixel 8, dispositivos modernos)
- [x] Ajuste autom√°tico del layout con el teclado (`adjustResize`)

### Capacidades de IA
- [x] Integraci√≥n de **Text-to-Speech (TTS)** con reproducci√≥n autom√°tica
- [x] Integraci√≥n de **Speech-to-Text (STT)** con permisos en runtime
- [x] Streaming de respuestas token por token en tiempo real
- [x] Persistencia de configuraci√≥n con SharedPreferences + JSON

### Experiencia de Usuario
- [x] Mensajes de bienvenida y retroalimentaci√≥n de estado
- [x] Indicadores visuales de estado (Listening, Processing, Thinking, Ready)
- [x] Manejo de errores con mensajes descriptivos
- [x] Opci√≥n de limpiar historial de chat

## Plan de Desarrollo

### ‚úÖ Fase 1: Estabilidad y Configuraci√≥n (COMPLETADA)
Base s√≥lida con interfaz conversacional, streaming, failover y soporte multi-endpoint.

---

### ‚úÖ Fase 1.5: Mejoras de Endpoints y Conectividad (COMPLETADA)
**Objetivo:** Soporte completo y optimizado para todos los tipos de endpoints de Ollama y servicios cloud.

#### 1.5.1 Soporte para Ollama.com (Cloud con API Key)
- [x] **Detectar tipo de endpoint Ollama** en el di√°logo de configuraci√≥n
- [x] **Selector de tipo de endpoint:** Ollama Cloud, Self-Hosted, OpenAI, etc.
- [x] **Configuraci√≥n espec√≠fica para Ollama Cloud:** URL predefinida, API Key obligatoria y link de registro.
- [x] **Implementar cach√© de modelos disponibles** (TTL de 1 hora) con `ModelCacheManager`.
- [x] **Manejo de errores HTTP espec√≠ficos** (401, 402, 429, 404) con mensajes claros.

#### 1.5.2 Soporte Mejorado para Ollama Self-Hosted
- [x] **Auto-descubrimiento de servidores** mediante escaneo de subred en puerto 11434.
- [x] **Bot√≥n "Scan"** integrado en el flujo de a√±adir endpoint.
- [x] **Validaci√≥n de conectividad** y formateo autom√°tico de URL local.

#### 1.5.3 Template de Endpoints Preconfigurados
- [x] **Bot√≥n "Templates"** en MainActivity.
- [x] **Plantillas para:** OpenAI, Ollama Cloud, Groq, Together.AI, Mistral AI y LocalAI.
- [x] **Configuraci√≥n autom√°tica:** URLs, ayuda contextual y requerimientos de API Key pre-cargados.

#### 1.5.4 Sistema de Health Check
- [x] **Background worker** (WorkManager) para monitoreo peri√≥dico.
- [x] **Indicadores visuales en tiempo real:** Punto de estado (Verde/Rojo) y latencia en ms en la lista de endpoints.
- [x] **Notificaciones push** si el endpoint principal queda fuera de l√≠nea.
- [x] **Modo Opcional:** Switch para activar/desactivar la monitorizaci√≥n constante.

---

### ‚úÖ Fase 2: Capacidades Ag√©nticas (COMPLETADA)
**Objetivo:** Transformar el asistente en un agente capaz de ejecutar acciones mediante function calling.

#### 2.1 Implementaci√≥n de Tools (Function Calling)
**Descripci√≥n:** Permitir que el modelo LLM invoque funciones nativas de Android para realizar acciones.

**Tareas:**
- [x] **Definir esquema de tools en JSON** seg√∫n especificaci√≥n OpenAI
- [x] **Crear clase `ToolRegistry`** para registrar y gestionar funciones disponibles
- [x] **Implementar `ToolExecutor`** que mapee llamadas del modelo a c√≥digo Kotlin
- [x] **Modificar `OpenAiClient`** para enviar tools en el request y procesar `tool_calls` en la respuesta
- [x] **Crear tools b√°sicos iniciales:** `send_sms`, `make_call`, `set_alarm`, `search_contacts`, `get_location`, `open_app`, `get_weather`, `read_notifications`.
- [x] **Sistema de Tool Gating:** Control de permisos y confirmaciones de usuario por herramienta
- [x] **UI de configuraci√≥n de Tools:** Switches para habilitar/deshabilitar y solicitar confirmaci√≥n

#### 2.2 Soporte para MCP (Model Context Protocol)
- [x] **Implementar cliente MCP** siguiendo la especificaci√≥n de Anthropic
- [x] **Crear adaptador MCP-to-Tools** que convierta servidores MCP en tools ejecutables
- [x] **Desarrollar MCPs b√°sicos:** Sistema de Archivos, Calendario, Notas.
- [x] **Soporte para MCP remoto:** Cliente HTTP con JSON-RPC 2.0
- [x] **UI de configuraci√≥n MCP:** Gesti√≥n de servidores MCP locales y remotos

#### 2.3 Procesamiento Local Avanzado
- [x] **STT H√≠brido:** Opci√≥n de STT nativo Android (Local) vs Cloud Whisper.
- [x] **Whisper On-Device (C++ / JNI):** Integrar `whisper.cpp` para transcripci√≥n local de alta precisi√≥n.
- [x] **Gemma 2B Mobile Actions:** Integrar MediaPipe LLM Inference API para razonamiento local.
- [x] **Sistema de Endpoints Locales:** Modelos locales como endpoints configurables (local_gemma, local_whisper)
- [x] **Arquitectura unificada:** Sin routing especial, modelos locales integrados en el flujo normal

#### 2.4 Refactorizaciones y Optimizaciones
- [x] **ChatController:** L√≥gica compartida de chat extra√≠da (~250 l√≠neas de c√≥digo duplicado eliminadas)
- [x] **Correcciones de seguridad:** 32 bugs cr√≠ticos corregidos (memory leaks, race conditions, null pointers)
- [x] **Sistema de permisos robusto:** Validaciones en todas las operaciones sensibles
- [x] **Cleanup de recursos:** Gesti√≥n autom√°tica de archivos temporales (audio, TTS)

---

### üîß Fase 2.5: Optimizaciones de Arquitectura (EN PROGRESO)
**Objetivo:** Mejorar la mantenibilidad, testabilidad y organizaci√≥n del c√≥digo.

#### 2.5.1 Extracci√≥n de Controladores
- [x] **TtsController:** Centralizar l√≥gica de TTS (System TTS + Cloud TTS con MediaPlayer)
- [x] **PermissionController:** Unificar manejo de permisos runtime en una clase reutilizable
- [x] **WhisperController:** Consolidar l√≥gica de grabaci√≥n y transcripci√≥n (local + cloud)

#### 2.5.2 Testing
- [x] **Tests unitarios para ChatController:** Verificar l√≥gica de streaming y failover
- [x] **Tests de integraci√≥n para ToolExecutor:** Verificar ejecuci√≥n correcta de herramientas
- [x] **Tests de MCP:** Verificar comunicaci√≥n con servidores locales y remotos

#### 2.5.3 Mejoras de UI/UX
- [x] **Indicadores de progreso:** Barra indeterminada durante tools y "Thinking"
- [x] **Historial persistente:** Guardar conversaciones en almacenamiento local
- [x] **Temas personalizables:** Dark mode, light mode, y temas custom
- [x] **Shortcuts de voz:** Activacion mediante palabras clave personalizadas
- [x] **Soporte Markdown:** Renderizado de respuestas del LLM
- [x] **Bubble de tools:** Nombres, cancelacion, icono de estadisticas y tokens
- [x] **Cancelar solicitud:** Boton de enviar alterna a cancelar
- [x] **Timeout configurable de tools**
- [x] **Autoscroll inteligente** al top de la respuesta
- [x] **Aprovechar ancho de chat** y reducir margenes
- [x] **Barra de estado con tokens/tiempo** por solicitud

#### 2.5.4 Opciones de Modelos Locales (PENDIENTE)
- [x] **UI de seleccion local:** Exponer opciones para modelos locales en ajustes
- [x] **Visibilidad de modelos descargados:** Mostrar estado y selector activo
- [x] **Preferencias por categoria:** STT/LLM/TTS locales con prioridad clara
- [x] **AICore status:** Mostrar disponibilidad del SDK/servicio de IA
- [x] **Descargas LiteRT/TFLite:** UI para descargar modelos locales
- [x] **Secciones locales:** AICore/LiteRT LM, TFLite, MediaPipe con cabeceras claras
- [x] **Botones de test por modelo:** STT (grabar + transcribir) y LLM (ping)
- [x] **Indicador de modelo en memoria:** Tama√±o y estado por tarjeta
- [ ] **Carga bajo demanda con eviction real:** Aun falta liberar instancias de modelos inactivos (solo tracking)

#### Notas recientes (estado actual)
- ‚úÖ **GPU delegate para Whisper:** Se incluye LiteRT GPU y se intenta cargar con GPU; si falla, hace fallback automatico a CPU.
- ‚úÖ **Crash por GpuDelegateFactory$Options:** Resuelto con dependencias LiteRT GPU (ya no reinicia al cargar).
- ‚ö†Ô∏è **Whisper GPU inestable:** En algunos dispositivos el delegate falla con "Error applying delegate" y se usa CPU (mas lento).
- ‚úÖ **LLM local TFLite y MediaPipe:** Tests implementados (ping) con inicializacion de modelos descargados.
- ‚ö†Ô∏è **LLM multimodal TFLite:** No soportado por tokenizer actual (SmolVLM requiere otro formato).
- ‚ö†Ô∏è **Modelos grandes y memoria:** Se removio el pre-check de memoria; el sistema decide y puede fallar en runtime si no hay RAM.
- ‚ö†Ô∏è **MediaPipe .task:** Algunos modelos pueden crashear en native (`libllm_inference_engine_jni.so`), requiere validar compatibilidad por modelo.
- ‚ö†Ô∏è **Alineamiento 16KB:** `libassistant.so` se ajusta con flags; libs externas siguen con warning si no estan alineadas.
- ‚ö†Ô∏è **STT whisper local sin GPU:** el modelo de whisper de tflite no esta corriendo en GPU; hay que diagnosticarlo y valorar alternativas para tener Stt local acelerado en tiempo real.
- ‚ö†Ô∏è **TTS/STT remoto sin diagnostico:** El TTS/STT remoto no esta bien probado ni instrumentado (faltan logs/errores claros).
- ‚ö†Ô∏è **Resultados inconsistentes en modelos locales:** Respuestas variables y comportamiento inestable; requiere diagnostico y trazas.

#### Pendientes inmediatos (modelos locales)
- [ ] **Eviction real de modelos:** Descargar instancias inactivas (liberar memoria, no solo tracking).
- [ ] **Validar modelos MediaPipe .task** con lista de modelos compatibles para evitar crashes.
- [ ] **Resolver tokenizer multimodal** (SmolVLM requiere tokenizer/modelos extra).
- [ ] **Confirmar soporte GPU Whisper por modelo/dispositivo** y ajustar delegate (o forzar CPU si el modelo no soporta GPU).
- [ ] **Diagnostico completo de TTS:** Verificar TTS remoto/local, latencias y rutas de error.
- [ ] **Estabilidad modelos locales:** Logs detallados y repros para entender inconsistencias.

#### Planes futuros (UX/arquitectura)
- [ ] **Categorias ‚Üí Agentes:** Reemplazar categorias por configuraciones de agentes con modelo + tools.
- [ ] **Curar catalogo de modelos:** Probar y seleccionar los que mejor funcionan; definir presets iniciales.
- [ ] **Telemetria en chat:** Mostrar consumo de memoria y estado de carga del modelo.
- [ ] **Mejor feedback de errores:** Mensajes claros, estados y recuperacion.
- [ ] **Limpieza y reordenamiento:** Eliminar codigo de pruebas y reorganizar para max eficiencia.

---

### üé® Fase 3: Procesamiento Multimodal (FUTURO)
**Objetivo:** Capacidades de visi√≥n, generaci√≥n de im√°genes y procesamiento de audio avanzado.
- [ ] **Vision API:** Permitir que el asistente "vea" la pantalla o la c√°mara.
- [ ] **Image Generation:** Integrar DALL-E / Stable Diffusion.
- [ ] **Audio Pro:** Soporte para modelos de voz expresivos (ElevenLabs, OpenAI Voice).

---

## üß† Nueva Arquitectura de IA Local - LiteRT + AICore

### Estado Actual (Legacy - A Migrar)
- ‚ö†Ô∏è **Whisper.cpp integrado:** Funcional pero requiere JNI complejo
- ‚ö†Ô∏è **Gemma 2B con MediaPipe:** Limitado, sin multimodal
- ‚úÖ **Arquitectura unificada:** Modelos locales como endpoints normales (mantener)

### Nueva Estrategia: LiteRT + AICore (Google Official Stack)

### Arquitectura LiteRT + AICore

**Stack Completo:**
```
Assistant App
    ‚Üì
ML Kit GenAI APIs / LiteRT-LM C++ (preview)
    ‚Üì
AICore (Android System Service)
    ‚Üì
LiteRT Runtime (ex-TFLite)
    ‚Üì
Hardware Acceleration (CPU/GPU/NPU)
```

**Beneficios:**
- ‚úÖ **Stack oficial de Google** - Soporte a largo plazo garantizado
- ‚úÖ **Aceleraci√≥n autom√°tica** - NPU en Tensor, Snapdragon, Dimensity (5x m√°s r√°pido)
- ‚úÖ **Sin JNI complejo** - Todo en Kotlin/Java con APIs high-level
- ‚úÖ **Actualizaciones v√≠a Play Services** - Sin recompilar la app
- ‚úÖ **Modelos pre-optimizados** - Colecci√≥n oficial en [HuggingFace LiteRT Community](https://huggingface.co/collections/litert-community/android-models)

### Modelos Seleccionados

**Fuente principal de modelos (HF):**
- https://huggingface.co/collections/litert-community/android-models

#### 1. LLM Multimodal: Gemma 3n
**Modelo:** [google/gemma-3n-E2B-it-litert-lm](https://huggingface.co/google/gemma-3n-E2B-it-litert-lm)

**Caracter√≠sticas:**
- üìä **Tama√±o:** ~2GB RAM (5B params con arquitectura eficiente)
- üéØ **Multimodal nativo:** Audio + Imagen + Video + Texto ‚Üí Texto
- üé§ **Audio encoder:** USM (Universal Speech Model)
- ‚ö° **NPU acceleration:** 5x m√°s r√°pido en Pixel 8 (Google Tensor G3)
- üîß **Capacidades:** Transcripci√≥n, traducci√≥n, razonamiento, visi√≥n

**Alternativa ligera:**
- [google/gemma-3n-E4B-it-litert-lm](https://huggingface.co/google/gemma-3n-E4B-it-litert-lm) - 3GB RAM, m√°s capaz

**Otros modelos disponibles:**
- **Qwen2.5-1.5B-Instruct** (1.5GB) - Excelente para chat general
- **Phi-4-mini-instruct** - Optimizado para razonamiento
- **SmolVLM-256M-Instruct** (256MB) - Vision-Language ultra-ligero
- **DeepSeek-R1-Distill-Qwen-1.5B** - Razonamiento mejorado

#### 2. STT: Whisper TFLite (DocWolle Collection)
**Repositorio:** [DocWolle/whisper_tflite_models](https://huggingface.co/DocWolle/whisper_tflite_models)
**Total:** 18 modelos optimizados (2.26GB en total)
**Licencia:** MIT
**Descargas:** 5,095/mes

##### Modelos por Tama√±o

| Tama√±o | Modelo | Peso | Idiomas | Recomendaci√≥n |
|--------|--------|------|---------|---------------|
| **Tiny** | `whisper-tiny.en.tflite` | 41.5 MB | Solo ingl√©s | ‚ö° Ultra r√°pido |
| **Tiny** | `whisper-tiny-transcribe-translate.tflite` | 42.1 MB | Multilingual | ‚≠ê **Balance ideal** |
| **Base** | `whisper-base-transcribe-translate.tflite` | 78.5 MB | Multilingual | ‚≠ê **Recomendado** |
| **Base** | `whisper-base.{es,de,fr,it,pt,ru,zh,hi,ur}.tflite` | 78.4 MB | 1 idioma | Especializado |
| **Base** | `whisper-base.EUROPEAN_UNION.tflite` | 94.8 MB | EU + Noruego | Para Europa |
| **Base** | `whisper-base.TOP_WORLD.tflite` | 108 MB | Top idiomas | Universal |
| **Small** | `whisper-small-transcribe-translate.tflite` | 249 MB | Multilingual | Alta precisi√≥n |
| **Small** | `whisper-small.TOP_WORLD.tflite` | 307 MB | Top idiomas | Mejor calidad |
| **Small** | `whisper-small.tflite` | 388 MB | Full multilingual | M√°xima capacidad |

##### Caracter√≠sticas T√©cnicas

**Input:**
- Shape: `(1, 80, 3000)` - Mel-spectrogram float32
- 80 frequency bins √ó 3000 time steps
- ~30 segundos de audio por inferencia

**Output:**
- Tokens generados (max 450 configurable)
- Dual signatures: `serving_transcribe` + `serving_translate`

**Modos de Operaci√≥n (Forced Decoder IDs):**
```kotlin
// Transcripci√≥n (cualquier idioma)
val transcribeIDs = arrayOf(
    intArrayOf(2, 50359),  // Modo transcribe
    intArrayOf(3, 50363)   // Sin timestamps
)

// Traducci√≥n al ingl√©s
val translateIDs = arrayOf(
    intArrayOf(2, 50358),  // Modo translate
    intArrayOf(3, 50363)   // Sin timestamps
)

// Transcripci√≥n espec√≠fica (ej: Espa√±ol)
val spanishTranscribeIDs = arrayOf(
    intArrayOf(1, 50262),  // Espa√±ol
    intArrayOf(2, 50359),  // Transcribe
    intArrayOf(3, 50363)   // Sin timestamps
)
```

**Decoder IDs Importantes:**
- `50358` - Traducir a ingl√©s
- `50359` - Transcribir (mantener idioma original)
- `50363` - Sin timestamps (m√°s r√°pido)
- `50261` - Alem√°n
- `50262` - Espa√±ol
- Ver [lista completa de idiomas](https://github.com/woheller69/whisperIME/blob/master/app/src/main/java/com/whispertflite/utils/InputLang.java)

**Archivos Adicionales:**
- `filters_vocab_en.bin` (586 KB) - Vocabulario ingl√©s
- `filters_vocab_multilingual.bin` (572 KB) - Vocabulario multilingual

##### Ventajas vs Whisper.cpp

| Caracter√≠stica | Whisper TFLite | Whisper.cpp (actual) |
|----------------|----------------|----------------------|
| Integraci√≥n | ‚úÖ Nativo Kotlin/Java | ‚ùå Requiere JNI |
| NPU Support | ‚úÖ Via LiteRT delegation | ‚ùå Solo CPU |
| Tama√±o APK | ‚úÖ Sin libs nativas | ‚ùå +20MB por ABI |
| Mantenimiento | ‚úÖ Oficial Google | ‚ö†Ô∏è Comunidad |
| Dual mode | ‚úÖ Transcribe + Translate | ‚ùå Solo transcribe |
| Latencia (Pixel 8) | ‚úÖ ~0.5s (tiny, NPU) | ~1s (tiny, CPU) |
| Idiomas espec√≠ficos | ‚úÖ 9 modelos optimizados | ‚ùå Solo multilingual |

##### Recomendaciones para Assistant

**Configuraci√≥n por defecto sugerida:**
1. **Primary:** `whisper-tiny-transcribe-translate.tflite` (42MB)
   - Raz√≥n: Ultra r√°pido, soporta transcribe + translate, multilingual
   - Latencia esperada: <1s en Pixel 8 con NPU

2. **Opci√≥n avanzada:** `whisper-base-transcribe-translate.tflite` (78MB)
   - Para usuarios que prefieren mayor precisi√≥n
   - Latencia esperada: ~1.5s en Pixel 8 con NPU

3. **Especializado espa√±ol:** `whisper-base.es.tflite` (78MB)
   - Si detectamos que el usuario habla principalmente espa√±ol
   - Mejor precisi√≥n para un idioma espec√≠fico

---

## Fase Actual: Implementaci√≥n Base LiteRT + TFLite

### Objetivo
Implementar infraestructura b√°sica para cargar y ejecutar modelos TFLite/LiteRT desde HuggingFace, permitiendo probar diferentes modelos y evaluar opciones.

### Alcance de esta Fase

**Implementaciones B√°sicas:**
1. ‚úÖ Setup de LiteRT + TFLite runtime
2. ‚úÖ Model downloader desde HuggingFace
3. ‚úÖ Carga y ejecuci√≥n de modelos TFLite (.tflite) y LiteRT (.litertlm)
4. ‚úÖ UI para seleccionar y probar diferentes modelos
5. ‚úÖ Integraci√≥n con arquitectura existente (endpoints locales)

**NO incluido en esta fase:**
- ‚ùå Optimizaciones avanzadas (NPU delegation, KV-cache, etc.)
- ‚ùå Features multimodales (audio/vision processing)
- ‚ùå Function calling (synthetic o dedicado)
- ‚ùå Routing inteligente cloud/local
- ‚ùå Fine-tuning de modelos

### Cat√°logo de Modelos Descargables

**Total: 17 modelos disponibles** (3.3MB - 3GB cada uno)

#### üì± STT - Whisper (7 modelos)

| Modelo | Tama√±o | Idiomas | Especializaci√≥n |
|--------|--------|---------|-----------------|
| Whisper Tiny - Multilingual | 42 MB | 100+ | Transcribe + Translate ‚≠ê Recomendado |
| Whisper Tiny - English Only | 41.5 MB | Solo EN | M√°xima velocidad |
| Whisper Base - Multilingual | 78.5 MB | 100+ | Balance ideal |
| **Whisper Base - Spanish** | 78.4 MB | **Solo ES** | **Especializado espa√±ol** |
| **Whisper Base - English** | 78.4 MB | **Solo EN** | **Especializado ingl√©s** |
| Whisper Small - Multilingual | 249 MB | 100+ | Alta precisi√≥n |

#### üé® LLM - Multimodal (3 modelos)

| Modelo | Tama√±o | Capacidades | Descripci√≥n |
|--------|--------|-------------|-------------|
| **Gemma 3n E2B** | **~2 GB** | **Audio + Image + Video + Text ‚Üí Text** | **‚≠ê Recomendado - USM encoder, NPU, 140+ idiomas** |
| Gemma 3n E4B | ~3 GB | Audio + Image + Video + Text ‚Üí Text | M√°s capaz que E2B |
| SmolVLM-256M | ~300 MB | Image + Text ‚Üí Text | Ultra ligero para visi√≥n |

**Nota:** Gemma 3n confirmado con soporte multimodal nativo seg√∫n [documentaci√≥n oficial de Google](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/).

#### üí¨ LLM - Text Only (4 modelos)

| Modelo | Tama√±o | Especializaci√≥n |
|--------|--------|-----------------|
| Gemma 3-270M | ~300 MB | Ultra ligero, chat simple |
| Qwen 2.5-1.5B | ~1.5 GB | Balance calidad/velocidad ‚≠ê Popular |
| Phi-4 Mini | ~1.8 GB | Razonamiento complejo |
| Gemma 2-2B | ~2 GB | Estable y confiable |

#### üîß Function Calling (1 modelo)

| Modelo | Tama√±o | Descripci√≥n |
|--------|--------|-------------|
| FunctionGemma-270M | ~288 MB | Especializado en function calling (requiere fine-tuning) |

**Uso total estimado si se descarga todo:** ~14.5 GB

### Plan de Implementaci√≥n

#### Paso 1: Setup de Dependencias

**A√±adir en `app/build.gradle`:**
```gradle
dependencies {
    // TensorFlow Lite (para modelos .tflite)
    implementation 'org.tensorflow:tensorflow-lite:2.15.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.15.0'
    implementation 'org.tensorflow:tensorflow-lite-support:0.4.4'

    // Google AI Edge LiteRT (para modelos .litertlm - opcional)
    implementation 'com.google.ai.edge.litert:litert-api:1.0.0-beta01'
    implementation 'com.google.ai.edge.litert:litert-support:1.0.0-beta01'

    // OkHttp para descargar modelos desde HuggingFace
    implementation 'com.squareup.okhttp3:okhttp:4.12.0'

    // WorkManager para descargas en background
    implementation 'androidx.work:work-runtime-ktx:2.9.0'
}
```

**A√±adir en `AndroidManifest.xml`:**
```xml
<uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE"
    android:maxSdkVersion="28" />
```

**Crear directorio de modelos:**
```
app/src/main/assets/models/   (para modelos bundled - opcional)
```

#### Paso 2: Model Downloader Service

**Crear `ModelDownloadManager.kt`:**
```kotlin
package com.sbf.assistant

import android.content.Context
import android.util.Log
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.withContext
import okhttp3.OkHttpClient
import okhttp3.Request
import java.io.File
import java.io.FileOutputStream

class ModelDownloadManager(private val context: Context) {
    private val client = OkHttpClient()
    private val modelsDir = File(context.filesDir, "models").apply { mkdirs() }

    data class ModelInfo(
        val name: String,
        val url: String,
        val filename: String,
        val sizeBytes: Long,
        val type: String,  // "tflite" or "litertlm"
        val category: String,  // "STT", "LLM-Multimodal", "LLM-Text", "Function-Calling"
        val description: String  // User-friendly description
    )

    data class ModelCategory(val name: String, val models: List<ModelInfo>)

    companion object {
        // ========== WHISPER STT MODELS ==========
        private val WHISPER_MODELS = listOf(
            // Tiny Models (Fastest)
            ModelInfo(
                name = "Whisper Tiny - Multilingual",
                url = "https://huggingface.co/DocWolle/whisper_tflite_models/resolve/main/whisper-tiny-transcribe-translate.tflite",
                filename = "whisper-tiny-transcribe-translate.tflite",
                sizeBytes = 42_100_000,  // 42MB
                type = "tflite",
                category = "STT",
                description = "Ultra r√°pido, transcribe + traducci√≥n, 100+ idiomas"
            ),
            ModelInfo(
                name = "Whisper Tiny - English Only",
                url = "https://huggingface.co/DocWolle/whisper_tflite_models/resolve/main/whisper-tiny.en.tflite",
                filename = "whisper-tiny.en.tflite",
                sizeBytes = 41_500_000,  // 41.5MB
                type = "tflite",
                category = "STT",
                description = "Especializado en ingl√©s, m√°xima velocidad"
            ),

            // Base Models (Balanced)
            ModelInfo(
                name = "Whisper Base - Multilingual",
                url = "https://huggingface.co/DocWolle/whisper_tflite_models/resolve/main/whisper-base-transcribe-translate.tflite",
                filename = "whisper-base-transcribe-translate.tflite",
                sizeBytes = 78_500_000,  // 78.5MB
                type = "tflite",
                category = "STT",
                description = "Balance ideal, transcribe + traducci√≥n, 100+ idiomas"
            ),
            ModelInfo(
                name = "Whisper Base - Spanish",
                url = "https://huggingface.co/DocWolle/whisper_tflite_models/resolve/main/whisper-base.es.tflite",
                filename = "whisper-base.es.tflite",
                sizeBytes = 78_400_000,  // 78.4MB
                type = "tflite",
                category = "STT",
                description = "Especializado en espa√±ol, mayor precisi√≥n"
            ),
            ModelInfo(
                name = "Whisper Base - English",
                url = "https://huggingface.co/DocWolle/whisper_tflite_models/resolve/main/whisper-base.en.tflite",
                filename = "whisper-base.en.tflite",
                sizeBytes = 78_400_000,  // 78.4MB
                type = "tflite",
                category = "STT",
                description = "Especializado en ingl√©s, mayor precisi√≥n"
            ),

            // Small Models (Best Quality)
            ModelInfo(
                name = "Whisper Small - Multilingual",
                url = "https://huggingface.co/DocWolle/whisper_tflite_models/resolve/main/whisper-small-transcribe-translate.tflite",
                filename = "whisper-small-transcribe-translate.tflite",
                sizeBytes = 249_000_000,  // 249MB
                type = "tflite",
                category = "STT",
                description = "Alta precisi√≥n, transcribe + traducci√≥n, 100+ idiomas"
            )
        )

        // ========== MULTIMODAL LLM MODELS ==========
        private val MULTIMODAL_MODELS = listOf(
            ModelInfo(
                name = "Gemma 3n E2B - Multimodal",
                url = "https://huggingface.co/google/gemma-3n-E2B-it-litert-lm/resolve/main/model.litertlm",
                filename = "gemma-3n-E2B-it.litertlm",
                sizeBytes = 2_000_000_000,  // ~2GB
                type = "litertlm",
                category = "LLM-Multimodal",
                description = "Audio + Image + Video + Text ‚Üí Text. USM encoder, 5B params, 2GB RAM. 140+ idiomas"
            ),
            ModelInfo(
                name = "Gemma 3n E4B - Multimodal",
                url = "https://huggingface.co/google/gemma-3n-E4B-it-litert-lm/resolve/main/model.litertlm",
                filename = "gemma-3n-E4B-it.litertlm",
                sizeBytes = 3_000_000_000,  // ~3GB
                type = "litertlm",
                category = "LLM-Multimodal",
                description = "Versi√≥n m√°s capaz de E2B, mejor calidad, 3GB RAM"
            ),
            ModelInfo(
                name = "SmolVLM-256M - Vision-Language",
                url = "https://huggingface.co/litert-community/SmolVLM-256M-Instruct/resolve/main/smolvlm-256m-instruct.tflite",
                filename = "smolvlm-256m-instruct.tflite",
                sizeBytes = 300_000_000,  // ~300MB (estimado)
                type = "tflite",
                category = "LLM-Multimodal",
                description = "Ultra ligero, Image + Text ‚Üí Text, ideal para an√°lisis visual r√°pido"
            )
        )

        // ========== TEXT-ONLY LLM MODELS ==========
        private val TEXT_LLM_MODELS = listOf(
            ModelInfo(
                name = "Gemma 3-270M Instruct",
                url = "https://huggingface.co/litert-community/gemma-3-270m-it/resolve/main/gemma-3-270m-it.tflite",
                filename = "gemma-3-270m-it.tflite",
                sizeBytes = 300_000_000,  // ~300MB
                type = "tflite",
                category = "LLM-Text",
                description = "Ultra ligero, r√°pido, bueno para chat simple"
            ),
            ModelInfo(
                name = "Qwen 2.5-1.5B Instruct",
                url = "https://huggingface.co/litert-community/Qwen2.5-1.5B-Instruct/resolve/main/qwen2.5-1.5b-instruct.tflite",
                filename = "qwen2.5-1.5b-instruct.tflite",
                sizeBytes = 1_500_000_000,  // ~1.5GB
                type = "tflite",
                category = "LLM-Text",
                description = "Excelente balance calidad/velocidad, muy popular"
            ),
            ModelInfo(
                name = "Phi-4 Mini Instruct",
                url = "https://huggingface.co/litert-community/Phi-4-mini-instruct/resolve/main/phi-4-mini-instruct.tflite",
                filename = "phi-4-mini-instruct.tflite",
                sizeBytes = 1_800_000_000,  // ~1.8GB (estimado)
                type = "tflite",
                category = "LLM-Text",
                description = "Optimizado para razonamiento complejo"
            ),
            ModelInfo(
                name = "Gemma 2-2B Instruct",
                url = "https://huggingface.co/litert-community/Gemma2-2B-IT/resolve/main/gemma2-2b-it.tflite",
                filename = "gemma2-2b-it.tflite",
                sizeBytes = 2_000_000_000,  // ~2GB
                type = "tflite",
                category = "LLM-Text",
                description = "Versi√≥n anterior de Gemma, estable y confiable"
            )
        )

        // ========== FUNCTION CALLING MODELS ==========
        private val FUNCTION_CALLING_MODELS = listOf(
            ModelInfo(
                name = "FunctionGemma-270M",
                url = "https://huggingface.co/google/functiongemma-270m-it/resolve/main/functiongemma-270m-it.tflite",
                filename = "functiongemma-270m-it.tflite",
                sizeBytes = 288_000_000,  // ~288MB
                type = "tflite",
                category = "Function-Calling",
                description = "Especializado en function calling, requiere fine-tuning con tu dataset"
            )
        )

        // Cat√°logo completo
        val AVAILABLE_MODELS = WHISPER_MODELS + MULTIMODAL_MODELS + TEXT_LLM_MODELS + FUNCTION_CALLING_MODELS

        // Categor√≠as para UI
        val MODEL_CATEGORIES = listOf(
            ModelCategory("STT - Whisper", WHISPER_MODELS),
            ModelCategory("LLM - Multimodal (Audio/Vision)", MULTIMODAL_MODELS),
            ModelCategory("LLM - Text Only", TEXT_LLM_MODELS),
            ModelCategory("Function Calling", FUNCTION_CALLING_MODELS)
        )
    }

    suspend fun downloadModel(
        modelInfo: ModelInfo,
        onProgress: (progress: Int) -> Unit
    ): Result<File> = withContext(Dispatchers.IO) {
        try {
            val outputFile = File(modelsDir, modelInfo.filename)

            // Si ya existe, retornar
            if (outputFile.exists() && outputFile.length() == modelInfo.sizeBytes) {
                return@withContext Result.success(outputFile)
            }

            val request = Request.Builder().url(modelInfo.url).build()
            val response = client.newCall(request).execute()

            if (!response.isSuccessful) {
                return@withContext Result.failure(Exception("Download failed: ${response.code}"))
            }

            val body = response.body ?: return@withContext Result.failure(Exception("Empty response"))
            val totalBytes = body.contentLength()

            FileOutputStream(outputFile).use { output ->
                body.byteStream().use { input ->
                    val buffer = ByteArray(8192)
                    var bytesRead: Int
                    var totalRead = 0L

                    while (input.read(buffer).also { bytesRead = it } != -1) {
                        output.write(buffer, 0, bytesRead)
                        totalRead += bytesRead

                        val progress = ((totalRead * 100) / totalBytes).toInt()
                        onProgress(progress)
                    }
                }
            }

            Result.success(outputFile)
        } catch (e: Exception) {
            Log.e(TAG, "Failed to download model", e)
            Result.failure(e)
        }
    }

    fun getInstalledModels(): List<ModelInfo> {
        return AVAILABLE_MODELS.filter { model ->
            val file = File(modelsDir, model.filename)
            file.exists()
        }
    }

    fun deleteModel(modelInfo: ModelInfo): Boolean {
        val file = File(modelsDir, modelInfo.filename)
        return file.delete()
    }

    fun getModelFile(filename: String): File? {
        val file = File(modelsDir, filename)
        return if (file.exists()) file else null
    }

    companion object {
        private const val TAG = "ModelDownloadManager"
    }
}
```

#### Paso 3: TFLite Model Service

**Crear `TFLiteModelService.kt`:**
```kotlin
package com.sbf.assistant

import android.content.Context
import android.util.Log
import org.tensorflow.lite.Interpreter
import org.tensorflow.lite.gpu.GpuDelegate
import java.io.File
import java.nio.ByteBuffer

class TFLiteModelService(private val context: Context) {
    private var interpreter: Interpreter? = null
    private var gpuDelegate: GpuDelegate? = null
    private var currentModel: String? = null

    fun loadModel(modelFile: File, useGpu: Boolean = true): Boolean {
        return try {
            release()

            val options = Interpreter.Options().apply {
                setNumThreads(4)
                if (useGpu) {
                    val delegate = GpuDelegate()
                    gpuDelegate = delegate
                    addDelegate(delegate)
                    Log.d(TAG, "GPU delegation enabled")
                }
            }

            interpreter = Interpreter(modelFile, options)
            currentModel = modelFile.name
            Log.d(TAG, "Model loaded: ${modelFile.name}")
            true
        } catch (e: Exception) {
            Log.e(TAG, "Failed to load model", e)
            // Retry without GPU if GPU failed
            if (useGpu) {
                Log.d(TAG, "Retrying without GPU...")
                return loadModel(modelFile, useGpu = false)
            }
            false
        }
    }

    fun runInference(input: Any): Any? {
        val interp = interpreter ?: return null

        return try {
            // Placeholder - implementar seg√∫n tipo de modelo
            // Para texto: input = tokenized text, output = tokens
            // Para audio: input = mel-spectrogram, output = tokens
            val output = Array(1) { IntArray(512) }  // Placeholder
            interp.run(input, output)
            output
        } catch (e: Exception) {
            Log.e(TAG, "Inference failed", e)
            null
        }
    }

    fun getInputShape(): IntArray? {
        return interpreter?.getInputTensor(0)?.shape()
    }

    fun getOutputShape(): IntArray? {
        return interpreter?.getOutputTensor(0)?.shape()
    }

    fun release() {
        interpreter?.close()
        interpreter = null
        gpuDelegate?.close()
        gpuDelegate = null
        currentModel = null
    }

    companion object {
        private const val TAG = "TFLiteModelService"
    }
}
```

#### Paso 4: UI para Gesti√≥n de Modelos

**A√±adir en `AssistantSettingsActivity.kt` o crear nueva `ModelManagementActivity.kt`:**

```kotlin
// A√±adir secci√≥n en Settings para modelos locales
class ModelManagementFragment : Fragment() {
    private lateinit var downloadManager: ModelDownloadManager
    private lateinit var adapter: ModelListAdapter

    override fun onCreateView(...): View {
        downloadManager = ModelDownloadManager(requireContext())

        // RecyclerView con lista de modelos disponibles
        val availableModels = ModelDownloadManager.AVAILABLE_MODELS
        val installedModels = downloadManager.getInstalledModels()

        adapter = ModelListAdapter(
            models = availableModels,
            installed = installedModels.map { it.filename }.toSet(),
            onDownload = { model ->
                lifecycleScope.launch {
                    downloadManager.downloadModel(model) { progress ->
                        // Update progress bar
                    }
                }
            },
            onDelete = { model ->
                downloadManager.deleteModel(model)
                adapter.notifyDataSetChanged()
            }
        )

        return view
    }
}
```

**Layout sugerido:**
```
Settings ‚Üí Local AI Models

  üì± STT - Whisper (7 available):
  ‚òë Whisper Tiny - Multilingual (42MB) [Delete] ‚≠ê
  ‚òê Whisper Tiny - English Only (41.5MB) [Download]
  ‚òê Whisper Base - Multilingual (78.5MB) [Download]
  ‚òê Whisper Base - Spanish (78.4MB) [Download]
  ‚òê Whisper Base - English (78.4MB) [Download]
  ‚òê Whisper Small - Multilingual (249MB) [Download]

  üé® LLM - Multimodal (3 available):
  ‚òê Gemma 3n E2B - Multimodal (2GB) [Download] ‚≠ê
      Audio + Image + Video + Text ‚Üí Text
  ‚òê Gemma 3n E4B - Multimodal (3GB) [Download]
  ‚òê SmolVLM-256M - Vision (300MB) [Download]

  üí¨ LLM - Text Only (4 available):
  ‚òê Gemma 3-270M (300MB) [Download]
  ‚òê Qwen 2.5-1.5B (1.5GB) [Download] ‚≠ê
  ‚òê Phi-4 Mini (1.8GB) [Download]
  ‚òê Gemma 2-2B (2GB) [Download]

  üîß Function Calling (1 available):
  ‚òê FunctionGemma-270M (288MB) [Download]

  [Total: 42MB used / 8GB available]
  [17 models available for download]
```

#### Paso 5: Integraci√≥n con Arquitectura Existente

**Modificar `SettingsManager.kt`:**
```kotlin
// A√±adir preferencias para modelos locales
fun getLocalSttModel(): String {
    return prefs.getString("local_stt_model", "") ?: ""
}

fun setLocalSttModel(filename: String) {
    prefs.edit().putString("local_stt_model", filename).apply()
}

fun getLocalLlmModel(): String {
    return prefs.getString("local_llm_model", "") ?: ""
}

fun setLocalLlmModel(filename: String) {
    prefs.edit().putString("local_llm_model", filename).apply()
}
```

**Actualizar `LocalGemmaService.kt` para usar TFLite:**
```kotlin
class LocalGemmaService(private val context: Context) {
    private val tfliteService = TFLiteModelService(context)
    private val downloadManager = ModelDownloadManager(context)
    private val settingsManager = SettingsManager(context)

    fun loadModel(): Boolean {
        val modelFilename = settingsManager.getLocalLlmModel()
        if (modelFilename.isBlank()) return false

        val modelFile = downloadManager.getModelFile(modelFilename)
        return if (modelFile != null) {
            tfliteService.loadModel(modelFile)
        } else {
            Log.e(TAG, "Model file not found: $modelFilename")
            false
        }
    }

    fun generateResponse(query: String): String? {
        if (tfliteService.interpreter == null) {
            if (!loadModel()) return null
        }

        // TODO: Implementar tokenization + inference + detokenization
        // Por ahora retornar placeholder
        return "Local model response (TFLite) - TODO: implement tokenization"
    }

    companion object {
        private const val TAG = "LocalGemmaService"
    }
}
```

**Crear `LocalWhisperTFLiteService.kt`:**
```kotlin
class LocalWhisperTFLiteService(private val context: Context) {
    private val tfliteService = TFLiteModelService(context)
    private val downloadManager = ModelDownloadManager(context)
    private val settingsManager = SettingsManager(context)

    fun transcribe(audioFile: File): String? {
        val modelFilename = settingsManager.getLocalSttModel()
        if (modelFilename.isBlank()) return null

        val modelFile = downloadManager.getModelFile(modelFilename)
        if (modelFile == null || !tfliteService.loadModel(modelFile)) {
            return null
        }

        // TODO: Implementar audio ‚Üí mel-spectrogram ‚Üí inference ‚Üí detokenization
        return "Transcription - TODO: implement mel-spectrogram conversion"
    }
}
```

### Cronograma Fase Actual

**Total: 1-2 d√≠as**

**D√≠a 1 (4-6 horas):**
- ‚úÖ A√±adir dependencias en build.gradle (30 min)
- ‚úÖ Implementar ModelDownloadManager (2 horas)
- ‚úÖ Implementar TFLiteModelService (1.5 horas)
- ‚úÖ Testing b√°sico: descargar y cargar modelo (1 hora)

**D√≠a 2 (3-4 horas):**
- ‚úÖ Crear UI para gesti√≥n de modelos (2 horas)
- ‚úÖ Integrar con LocalGemmaService (1 hora)
- ‚úÖ Testing E2E: UI ‚Üí Download ‚Üí Load ‚Üí Inference placeholder (1 hora)

**Resultado esperado:**
- App puede descargar modelos desde HuggingFace
- App puede cargar modelos TFLite/LiteRT en memoria
- UI permite seleccionar modelo activo
- Placeholder de inferencia funciona (sin tokenization real todav√≠a)

### Checklist de Implementaci√≥n

- [ ] A√±adir dependencias TFLite + OkHttp + WorkManager
- [ ] Crear ModelDownloadManager con cat√°logo de modelos
- [ ] Implementar download con progress tracking
- [ ] Crear TFLiteModelService para cargar modelos
- [ ] A√±adir GPU delegation con fallback a CPU
- [ ] Crear UI de gesti√≥n de modelos (RecyclerView)
- [ ] Integrar con SettingsManager (preferencias de modelo)
- [ ] Actualizar LocalGemmaService para usar TFLite
- [ ] Crear LocalWhisperTFLiteService (placeholder)
- [ ] Testing: Download ‚Üí Load ‚Üí Basic inference
- [ ] Compilar y probar en dispositivo f√≠sico

---

## Sugerencias para Fases Futuras

### Fase Futura 1: Tokenization y Detokenization

**Objetivos:**
- Implementar conversi√≥n audio ‚Üí mel-spectrogram para Whisper
- Implementar tokenizer/detokenizer para modelos de texto
- Cargar vocabularios (.bin files)

**Componentes:**
- Audio processing pipeline (TarsosDSP o custom FFT)
- Vocabulary parser (filters_vocab_multilingual.bin)
- BPE tokenizer para Gemma

### Fase Futura 2: Optimizaciones de Performance

**NPU Acceleration:**
- Implementar LiteRT delegation para NPU (Tensor, Snapdragon, Dimensity)
- Benchmark latencia: CPU vs GPU vs NPU
- Auto-selection basado en hardware disponible

**KV-Cache:**
- Implementar KV-cache para reducir latencia en conversaciones
- Memory pooling para evitar allocations

**Model Quantization:**
- Evaluar modelos 8-bit vs 4-bit vs float16
- Trade-off precisi√≥n vs velocidad vs tama√±o

### Fase Futura 3: Function Calling

**HALLAZGO:** Gemma 3n NO soporta function calling nativo. Tres opciones:

#### Opci√≥n A: Synthetic Function Calling
- Implementar sobre Gemma 3n usando system prompt + JSON parsing
- **Pros:** Un solo modelo, sin fine-tuning
- **Contras:** Menor precisi√≥n, propenso a errores de formato

#### Opci√≥n B: FunctionGemma 270M Dedicado
- Usar [google/functiongemma-270m-it](https://huggingface.co/google/functiongemma-270m-it)
- Fine-tuning con dataset Mobile Actions + herramientas custom
- **Pros:** Function calling nativo y optimizado, solo 288MB
- **Contras:** Requiere fine-tuning, modelo adicional

#### Opci√≥n C: Arquitectura Dual (Recomendada)
- FunctionGemma 270M ‚Üí Detecci√≥n de tool calls
- Gemma 3n ‚Üí Razonamiento multimodal + respuesta final
- **Pros:** Mejor de ambos mundos
- **Contras:** M√°s complejo, ~2.3GB RAM total

```kotlin
class LocalModelRouter(context: Context) {
    private val functionGemma = FunctionGemmaService(context)  // 288MB
    private val gemma3n = Gemma3nService(context)              // ~2GB

    suspend fun processQuery(query: String, tools: List<Tool>): Response {
        // 1. FunctionGemma detecta tool calls
        val functionCall = functionGemma.detectFunctionCall(query, tools)

        if (functionCall != null) {
            // 2. Ejecutar tool
            val toolResult = executeTools(functionCall)

            // 3. Gemma 3n genera respuesta final
            return gemma3n.generate(context = toolResult, query = query)
        } else {
            // 4. Respuesta directa (multimodal)
            return gemma3n.generate(query)
        }
    }
}
```

### Fase Futura 4: Multimodal

**Audio Directo a Gemma 3n:**
- Bypass STT: enviar audio raw directamente a Gemma 3n
- Usar USM (Universal Speech Model) encoder
- Comparar latencia vs STT ‚Üí Texto ‚Üí LLM

**Vision con Gemma 3n:**
- Captura de pantalla para contexto visual
- Camera input para "ver" el mundo
- Integrar con tool "analyze_screen"

**Video Input:**
- Procesar frames de video
- An√°lisis temporal de secuencias

### Fase Futura 5: Routing Inteligente Cloud/Local

**Heur√≠sticas:**
```kotlin
fun shouldUseLocal(query: String, battery: Int, connection: Boolean): Boolean {
    return when {
        !connection -> true // Sin internet ‚Üí forzar local
        battery < 20 -> query.length < 100 // Bater√≠a baja ‚Üí solo queries simples
        else -> query.length < 200 // Default: queries cortas locales
    }
}
```

**Features:**
- UI: Switch manual "Preferir local" / "Preferir cloud" / "Autom√°tico"
- M√©tricas: Loggear latencia, precisi√≥n, satisfacci√≥n
- Fallback autom√°tico: Local falla ‚Üí Cloud

### Fase Futura 6: Model Marketplace

**Caracter√≠sticas:**
- Browse de modelos desde HuggingFace Collections
- Filtros: Tama√±o, task, language, rating
- Reviews y benchmarks comunitarios
- One-click install + warm-up autom√°tico

### Fase Futura 7: Fine-tuning On-Device

**Caracter√≠sticas:**
- LoRA adapters para personalizaci√≥n
- Dataset collection desde conversaciones del usuario
- Privacy-preserving fine-tuning
- A/B testing de adapters

### Recursos y Referencias

**Documentaci√≥n Oficial:**
- [LiteRT Documentation](https://ai.google.dev/edge/litert)
- [Gemma Mobile Actions](https://ai.google.dev/gemma/docs/mobile-actions)
- [FunctionGemma Guide](https://ai.google.dev/gemma/docs/functiongemma)
- [TensorFlow Lite Guide](https://www.tensorflow.org/lite/guide)

**Modelos:**
- [LiteRT Community Collection](https://huggingface.co/collections/litert-community/android-models)
- [Whisper TFLite Models](https://huggingface.co/DocWolle/whisper_tflite_models)
- [FunctionGemma](https://huggingface.co/google/functiongemma-270m-it)

**C√≥digo de Referencia:**
- [LiteRT-LM GitHub](https://github.com/google-ai-edge/LiteRT-LM)
- [Gemma Cookbook](https://github.com/google-gemini/gemma-cookbook)
### Requisitos M√≠nimos

**Hardware:**
- Android 14+ (para AICore API completa)
- 4GB RAM m√≠nimo (para E2B)
- 8GB storage libre (modelos + cache)
- Recomendado: Tensor G3, Snapdragon 8 Gen 3, Dimensity 9300 (NPU support)

**Software:**
- Google Play Services actualizado
- Permisos: INTERNET, WRITE_EXTERNAL_STORAGE, RECORD_AUDIO
